{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112082668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the game we'll be working on. The player can move the blue square up, down, left or right. The point of the game is to get to the green square (+1 reward) and avoid the red square (-1 reward). Blocks are randomized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, final_layer_size):\n",
    "        # The input image of the game is 84 x 84 x 3 (RGB) \n",
    "        inputs = Input(shape=env.state.shape())\n",
    "\n",
    "        # There will be four layers of convolutions performed on the image input\n",
    "        # A convolution take a portion of an input and matrix multiplies\n",
    "        # a filter on the portion to get a new input (see below)\n",
    "        self.model = Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[8,8],\n",
    "            strides=[4,4],\n",
    "            padding=\"valid\")(inputs)\n",
    "        self.model = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4,4],\n",
    "            strides=[2,2],\n",
    "            padding=\"valid\")(self.model)\n",
    "        self.model = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3,3],\n",
    "            strides=[1,1],\n",
    "            padding=\"valid\")(self.model)\n",
    "        self.model = Conv2D(\n",
    "            filters= final_layer_size,\n",
    "            kernel_size=[7,7],\n",
    "            strides=[1,1],\n",
    "            padding=\"valid\")(self.model)\n",
    "\n",
    "        \n",
    "        # We then separate the final convolution layer into an advantage and value\n",
    "        # stream. The value function is how well off you are in a given state.\n",
    "        # The advantage is the how much better off you are after making a particular\n",
    "        # move. Q is the value function of a state after a given action.\n",
    "        # Advantage(state, action) = Q(state, action) - Value(state)\n",
    "        self.stream_AC, self.stream_VC = tf.split(split_dim=3,num_split=2,value=self.conv4)\n",
    "        \n",
    "        # We then flatten the advantage and value functions\n",
    "        self.stream_AC = slim.flatten(self.stream_AC)\n",
    "        self.stream_VC = slim.flatten(self.stream_VC)\n",
    "        \n",
    "        # We define weights for our advantage and value layers. We will train these\n",
    "        # layers so the matmul will match the expected value and advantage from play\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([final_layer_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([final_layer_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.stream_AC, self.AW)\n",
    "        self.Value = tf.matmul(self.stream_VC, self.VW)\n",
    "        \n",
    "        # To get the Q output, we need to add the value to the advantage.\n",
    "        # The advantage to be evaluated will bebased on how good the action\n",
    "        # is based on the average advantage of that state\n",
    "        self.Q_out = self.Value + tf.subtract(self.Advantage,\n",
    "                                              tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "\n",
    "        # Our prediction is then the highest Q output\n",
    "        self.predict = tf.argmax(self.Q_out, 1)\n",
    "        \n",
    "        # We need to keep track of our losses (sum of square differences)\n",
    "        self.target_Q = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions,dtype=tf.float32)\n",
    "        \n",
    "        # Final Q values that we want to learn from\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Q_out, self.actions_onehot), axis=1)\n",
    "        \n",
    "        # Calculating errors and training\n",
    "        self.td_error = tf.square(self.target_Q - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_model = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How convolutions work\n",
    "![How convolutions work](img/convolutions.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "This class allows us to store experiences and sample them randomly for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self,buffer_size=50000):\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        # Extend the stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # Keep the last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "        \n",
    "    def sample(self, size):\n",
    "        sample_idxs = np.random.randint(len(self.buffer),size=size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output,(size,-1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(states):\n",
    "    # This will flatten our state to be used in our Q network\n",
    "    return states.flatten()\n",
    "\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "# def update_target_graph(tf_vars, tau):\n",
    "#     num_vars = len(tf_vars)\n",
    "#     # tf_vars contains both our primary and target graph\n",
    "#     # [primary..., target...]\n",
    "#     # target graph starts half way through the list\n",
    "#     target_start = num_vars // 2\n",
    "#     op_holder = []\n",
    "#     for idx, var in enumerate(tf_vars[:target_start]):\n",
    "#         # ??\n",
    "# #         updated_val = (var.value() * tau) + \\\n",
    "#         updated_val = (var * tau) + \\\n",
    "#             ((1 - tau) * tf_vars[target_start + idx])\n",
    "# #             ((1 - tau) * tf_vars[target_start + idx].value())\n",
    "# #         updated_graph = tf_vars[target_start + idx]\n",
    "#     #         updated_graph.assign(updated_val)\n",
    "#         op_holder.append(updated_val)\n",
    "#     return op_holder\n",
    "\n",
    "def update_target(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5, 5.5, 6.5, 7.5, 8.5]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_target_graph(np.arange(10),0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # How many experiences to use for each training step\n",
    "update_freq = 4 # How often to perform a training step\n",
    "y = 0.99 # Discount factor\n",
    "prob_random_start = 1 # Starting chance of random action\n",
    "prob_random_end = 0.1 # Ending chance of random action\n",
    "annealing_steps = 10000. # Steps of training to reduce from start_e -> end_e\n",
    "num_episodes = 10000 # How many episodes of game environment to train\n",
    "pre_train_episodes = 100 # Number of episodes of random actions\n",
    "max_num_step = 50 # Maximum allowed episode length\n",
    "load_model = False # Whether to load a saved model\n",
    "path = \"./dqn\" # Path to save our model to\n",
    "final_layer_size = 512 # Size of the final convolution layer before \n",
    "                       # splitting into Advantage and Value streams\n",
    "tau = 0.001 # Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episode: 0 Mean reward: 2.0000\n",
      "Num episode: 50 Mean reward: 1.8200\n",
      "Num episode: 100 Mean reward: 1.5400\n"
     ]
    }
   ],
   "source": [
    "# Reset everything\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setup our Q-networks\n",
    "main_qn = Qnetwork(final_layer_size)\n",
    "target_qn = Qnetwork(final_layer_size)\n",
    "\n",
    "# Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver to save our neural network\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Variables we'll be training in the neural network\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "# Returns our variables\n",
    "target_ops = updateTargetGraph(trainables, tau)\n",
    "\n",
    "# Setup our experience replay\n",
    "experience_replay = ExperienceReplay()\n",
    "\n",
    "# We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "# we will begin reducing the probability of acting randomly, and instead\n",
    "# take the actions that our Q network suggests\n",
    "prob_random = prob_random_start\n",
    "prob_random_drop = (prob_random_start - prob_random_end) / annealing_steps\n",
    "\n",
    "num_steps = [] # Tracks number of steps per episode\n",
    "rewards = [] # Tracks rewards per episode\n",
    "total_steps = 0 # Tracks cumulative steps taken in training\n",
    "\n",
    "print_every = 50 # How often to print status\n",
    "save_every = 10 # How often to save\n",
    "\n",
    "# Setup path for saving\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print(\"Loading model...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for num_episode in range(num_episodes):\n",
    "        # Create an experience replay for the current episode\n",
    "        episode_buffer = ExperienceReplay()\n",
    "        \n",
    "        # Get the game state from the environment\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Process the game state so it can be used in our Q network\n",
    "        state = process_state(state)\n",
    "        \n",
    "        done = False # Game is complete\n",
    "        sum_rewards = 0 # Running sum of rewards in episode\n",
    "        cur_step = 0 # Running sum of number of steps taken in episode\n",
    "        \n",
    "        while cur_step < max_num_step:\n",
    "            cur_step += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            if np.random.rand() < prob_random or \\\n",
    "                num_episode < pre_train_episodes:\n",
    "                    # Act randomly based on prob_random or if we\n",
    "                    # have not accumulated enough pre_train steps\n",
    "                    action = np.random.randint(env.actions)\n",
    "            else:\n",
    "                # Decide what action to take from the Q network\n",
    "                feed_dict = {main_qn.scalar_input: [state]}\n",
    "                action = sess.run(main_qn.predict,\n",
    "                                  feed_dict=feed_dict)\n",
    "                action = action[0]\n",
    "            \n",
    "            # Take the action and retrieve the next state, reward and done\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Process next state\n",
    "            next_state = process_state(next_state)\n",
    "            \n",
    "            # Setup the episode to be stored in the episode buffer\n",
    "            episode = np.array([state,action,reward,next_state,done])\n",
    "            episode = episode.reshape(1,-1)\n",
    "            episode_buffer.add(episode)\n",
    "            \n",
    "            sum_rewards += reward\n",
    "            state = next_state\n",
    "\n",
    "            if num_episode > pre_train_episodes:\n",
    "                if prob_random > prob_random_end:\n",
    "                    prob_random -= prob_random_drop\n",
    "                \n",
    "                if total_steps % update_freq == 0:\n",
    "                    # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                    train_batch = experience_replay.sample(batch_size)\n",
    "                    # Separate the batch into its components\n",
    "                    train_state, train_action, train_reward, \\\n",
    "                        train_next_state, train_done = train_batch.T\n",
    "                    \n",
    "                    # Convert state and next_state from ???\n",
    "                    train_state = np.vstack(train_state)\n",
    "                    train_next_state = np.vstack(train_next_state)\n",
    "                    \n",
    "                    # Our predictions (actions to take) from the main Q network\n",
    "                    train_next_state_action = sess.run(\n",
    "                        main_qn.predict,\n",
    "                        feed_dict = { \n",
    "                          main_qn.scalar_input: train_state\n",
    "                        })\n",
    "                    \n",
    "                    # The Q values from our target network from the next state\n",
    "                    train_next_state = sess.run(target_qn.Q_out,\n",
    "                                 feed_dict = {\n",
    "                                     target_qn.scalar_input: train_next_state\n",
    "                                 })\n",
    "                    \n",
    "                    # Tells us whether game over or not\n",
    "                    # We will multiply our gradients by this value\n",
    "                    # to ensure we don't train on the last move\n",
    "                    train_gameover = train_done == 0\n",
    "                    \n",
    "                    # Q value of the next state based on action\n",
    "                    train_next_state_values = train_next_state[range(batch_size), train_next_state_action]\n",
    "                    \n",
    "                    # Reward from the action chosen in the train batch\n",
    "                    target_q = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                    \n",
    "                    _ = sess.run(main_qn.update_model, \\\n",
    "                                feed_dict={\n",
    "                                    main_qn.scalar_input: train_state,\n",
    "                                    main_qn.target_Q: target_q,\n",
    "                                    main_qn.actions: train_action\n",
    "                                })\n",
    "                    \n",
    "                    update_target(target_ops, sess)\n",
    "            if done:\n",
    "                break\n",
    "        experience_replay.add(episode_buffer.buffer)\n",
    "        num_steps.append(cur_step)\n",
    "        rewards.append(sum_rewards)\n",
    "        \n",
    "        if num_episode % save_every == 0:\n",
    "            saver.save(sess, path + \"/model-{}.ckpt\".format(num_episode))\n",
    "        if num_episode % print_every == 0:\n",
    "            print(\"Num episode: {} Mean reward: {:0.4f}\".format(\n",
    "                num_episode, np.mean(rewards[-print_every:])\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "step: 1 action: 1 reward: 0.0 total rewards: 0.0\n",
      "step: 2 action: 2 reward: 0.0 total rewards: 0.0\n",
      "step: 3 action: 2 reward: -1.0 total rewards: -1.0\n",
      "step: 4 action: 0 reward: 0.0 total rewards: -1.0\n",
      "step: 5 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 6 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 7 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 8 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 9 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 10 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 11 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 12 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 13 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 14 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 15 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 16 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 17 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 18 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 19 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 20 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 21 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 22 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 23 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 24 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 25 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 26 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 27 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 28 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 29 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 30 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 31 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 32 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 33 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 34 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 35 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 36 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 37 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 38 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 39 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 40 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 41 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 42 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 43 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 44 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 45 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 46 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 47 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 48 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 49 action: 2 reward: 0.0 total rewards: -1.0\n",
      "step: 50 action: 2 reward: 0.0 total rewards: -1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU+t3rfK80AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e5a4b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Loading model...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    done = False\n",
    "    num_step = 0\n",
    "    sum_rewards = 0\n",
    "    state = process_state(env.reset())\n",
    "\n",
    "    while not done and num_step < max_num_step:\n",
    "        num_step += 1\n",
    "        action = sess.run(main_qn.predict, feed_dict = {\n",
    "            main_qn.scalar_input: [state]})[0]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        sum_rewards += reward\n",
    "        print(\"step: {} action: {} reward: {} total rewards: {}\".format(\n",
    "            num_step,action,reward,sum_rewards))\n",
    "        plt.imshow(env.renderEnv())\n",
    "        \n",
    "        state = process_state(next_state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.renderEnv().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
