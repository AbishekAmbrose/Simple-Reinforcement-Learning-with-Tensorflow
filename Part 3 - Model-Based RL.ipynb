{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-26 20:27:22,448] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 10\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # Discount factor for rewards\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad**2\n",
    "resume = False\n",
    "\n",
    "model_batch_size = 3\n",
    "real_batch_size = 3\n",
    "\n",
    "dimen = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_buffer(buffer):\n",
    "    return np.array([np.zeros_like(var) for var in buffer])\n",
    "\n",
    "def discount(r, gamma=0.99, standardize=False):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
    "    \"\"\"\n",
    "    discounted = np.array([val * (gamma ** i) for i, val in enumerate(r)])\n",
    "    if standardize:\n",
    "        discounted -= np.mean(discounted)\n",
    "        discounted /= np.std(discounted)\n",
    "    return discounted\n",
    "\n",
    "def step_model(sess, xs, action):\n",
    "    \"\"\" Uses our model to produce a new state given a previous state and action \"\"\"\n",
    "    # Last state\n",
    "    x = xs[-1].reshape(1,-1)\n",
    "    \n",
    "    # Append action\n",
    "    x = np.hstack([x, [[action]]])\n",
    "    \n",
    "    # Predict output\n",
    "    output_y = sess.run(predicted_state_m, feed_dict={input_x_m: x})\n",
    "    \n",
    "    # predicted_state_m == [state_0, state_1, state_2, state_3, reward, done]\n",
    "    output_next_state = output_y[:,:4]\n",
    "    output_reward = output_y[:,4]\n",
    "    output_done = output_y[:,5]\n",
    "    \n",
    "    # First and third env outputs are limited to +/- 2.4 and +/- 0.4\n",
    "    output_next_state[:,0] = np.clip(output_next_state[:,0],-2.4,2.4)\n",
    "    \n",
    "    output_next_state[:,2] = np.clip(output_next_state[:,2],-0.4,0.4)\n",
    "    \n",
    "    # Threshold for being done is likliehood being > 0.1\n",
    "    output_done = True if output_done > 0.01 or len(xs) > 500 else False\n",
    "    \n",
    "    return output_next_state, output_reward, output_done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "Here we define a neural network to determine policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_x_p = tf.placeholder(tf.float32, [None,4], name=\"input_x\")\n",
    "\n",
    "# First layer\n",
    "W1_p = tf.get_variable(\"W1\", shape=[dimen,num_hidden], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1_p = tf.nn.relu(tf.matmul(input_x_p, W1_p))\n",
    "\n",
    "# Second layer\n",
    "W2_p = tf.get_variable(\"W2\", shape=[num_hidden,1], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "output_p = tf.nn.sigmoid(tf.matmul(layer1_p, W2_p))\n",
    "\n",
    "trainable_vars_p = [W1_p, W2_p]\n",
    "input_y_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"input_y\")\n",
    "advantages_p = tf.placeholder(tf.float32, shape=[None,1], name=\"reward_signal\")\n",
    "\n",
    "# Gradients\n",
    "W1_grad_p = tf.placeholder(tf.float32,name=\"W1_grad\")\n",
    "W2_grad_p = tf.placeholder(tf.float32,name=\"W2_grad\")\n",
    "batch_grad_p = [W1_grad_p, W2_grad_p]\n",
    "\n",
    "# Loss function\n",
    "log_lik_p = tf.log(input_y_p * (input_y_p - output_p) + \n",
    "                 (1 - input_y_p) * (input_y_p + output_p))\n",
    "loss_p = -tf.reduce_mean(log_lik_p * advantages_p)\n",
    "\n",
    "# Gradients\n",
    "grads_p = tf.gradients(loss_p, trainable_vars_p)\n",
    "\n",
    "# Optimizer\n",
    "adam_p = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Update function\n",
    "update_grads_p = adam_p.apply_gradients(\n",
    "    zip(batch_grad_p, [W1_p, W2_p]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward and done state from a current state and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden_m = 256\n",
    "\n",
    "# Dimensions of the previous state plus 1 for the action\n",
    "dimen_m = dimen + 1\n",
    "\n",
    "input_x_m = tf.placeholder(tf.float32, [None, dimen_m])\n",
    "\n",
    "# previous_state = tf.placeholder(tf.float32, [None, dimen + 1], \n",
    "#                                 name=\"previous_state\")\n",
    "\n",
    "# First layer\n",
    "W1_m = tf.get_variable(\"W1_m\", shape=[dimen_m, num_hidden_m],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B1M\")\n",
    "layer1_m = tf.nn.relu(tf.matmul(input_x_m, W1_m) + B1_m)\n",
    "\n",
    "# Second layer\n",
    "W2_m = tf.get_variable(\"W2_m\", shape=[num_hidden_m, num_hidden_m],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B2_m\")\n",
    "layer2_m = tf.nn.relu(tf.matmul(layer1_m, W2_m) + B2_m)\n",
    "\n",
    "# Third (output) layers\n",
    "W_obs_m = tf.get_variable(\"W_obs_m\", shape=[num_hidden_m, 4],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_obs_m = tf.Variable(tf.zeros([4]), name=\"B_obs_m\")\n",
    "\n",
    "W_reward_m = tf.get_variable(\"W_reward_m\", shape=[num_hidden_m, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_reward_m = tf.Variable(tf.zeros([1]), name=\"B_reward_m\")\n",
    "\n",
    "W_done_m = tf.get_variable(\"W_done_m\", shape=[num_hidden_m,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_done_m = tf.Variable(tf.zeros([1]), name=\"B_done_m\")\n",
    "\n",
    "output_obs_m = tf.matmul(layer2_m, W_obs_m) + B_obs_m\n",
    "output_reward_m = tf.matmul(layer2_m, W_reward_m) + B_reward_m\n",
    "output_done_m = tf.sigmoid(tf.matmul(layer2_m, W_done_m) + B_done_m)\n",
    "\n",
    "# Placeholders for inputs\n",
    "actual_obs_m = tf.placeholder(tf.float32, [None, dimen_m], name=\"actual_obs\")\n",
    "actual_reward_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_reward\")\n",
    "actual_done_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_done\")\n",
    "\n",
    "predicted_state_m = tf.concat(1,[output_obs_m, output_reward_m, output_done_m])\n",
    "\n",
    "# Loss functions\n",
    "loss_obs_m = tf.square(actual_reward_m - output_reward_m)\n",
    "loss_reward_m = tf.square(actual_reward_m - output_reward_m)\n",
    "loss_done_m = -tf.log(actual_done_m * actual_done_m + \n",
    "                (1 - actual_done_m) * (1 - output_done_m))\n",
    "\n",
    "loss_m = tf.reduce_mean(loss_obs_m + loss_reward_m + loss_done_m)\n",
    "\n",
    "adam_m = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "update_m = adam_m.minimize(loss_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1500 Training model loss: 0.00048257794696837664\n",
      "Episode: 3000 Training model loss: 0.04406725987792015\n",
      "Episode: 4500 Training model loss: 2.730608503043186e-05\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "num_episode = 0\n",
    "num_episodes = 5000\n",
    "\n",
    "batch_size = real_batch_size\n",
    "\n",
    "# Setup array to keep track of preivous states and outputs\n",
    "observations = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "dones = np.empty(0).reshape(0,1)\n",
    "grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "observation = env.reset()\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    observation = np.reshape(observation, (1,-1))\n",
    "    \n",
    "    # Chose a random action\n",
    "    action = np.random.choice(range(env.action_space.n))\n",
    "    \n",
    "    # Append observation and actions\n",
    "    observations = np.vstack([observations, observation])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    dones = np.vstack([dones, done])\n",
    "    \n",
    "    # If game is done\n",
    "    if done or len(observations) > 300:\n",
    "        \n",
    "        # Count the episode\n",
    "        num_episode += 1\n",
    "        \n",
    "\n",
    "        # Previous state and actions for training model\n",
    "        states = np.hstack([observations, actions])\n",
    "        prev_states = states[:-1,:]\n",
    "        next_states = states[1:, :]\n",
    "        next_rewards = rewards[1:, :]\n",
    "        next_dones = dones[1:, :]\n",
    "\n",
    "        # Putting it all together\n",
    "        next_state_all = np.hstack([next_states, next_rewards, next_dones])\n",
    "\n",
    "        feed_dict = {input_x_m: prev_states.astype(np.float32), \n",
    "                     actual_obs_m: next_states.astype(np.float32),\n",
    "                    actual_done_m: next_dones.astype(np.float32),\n",
    "                    actual_reward_m: next_rewards.astype(np.float32)}\n",
    "        loss, output_state, _ = sess.run([loss_m, predicted_state_m, update_m], \n",
    "                                         feed_dict=feed_dict)\n",
    "        \n",
    "        observation = env.reset()\n",
    "        \n",
    "        # If the batch is full\n",
    "        if num_episode % batch_size == 0:\n",
    "            \n",
    "            if (num_episode % (100 * batch_size) == 0):\n",
    "                print(\"Episode: {} Training model loss: {}\".format(num_episode, loss))\n",
    "\n",
    "            if loss < 1e-6:\n",
    "                print(\"Training done! Episode: {} Model loss: {}\".format(num_episode, loss))\n",
    "                break\n",
    "                            \n",
    "            # Reset everything\n",
    "            observations = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            rewards = np.empty(0).reshape(0,1)\n",
    "            dones = np.empty(0).reshape(0,1)\n",
    "            grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 rewards: 10.0\n",
      "Episode 300 rewards: [ 133.38696289]\n",
      "Episode 600 rewards: [ 135.93412781]\n",
      "Episode 900 rewards: [ 141.17945862]\n",
      "Episode 1200 rewards: [ 179.2141571]\n",
      "Episode 1500 rewards: [ 191.54997253]\n",
      "Episode 1800 rewards: [ 175.94616699]\n",
      "Episode 2100 rewards: [ 172.42500305]\n",
      "Episode 2142 Training complete with total score of: 230.0\n"
     ]
    }
   ],
   "source": [
    "reward_sum = 0\n",
    "reward_total = []\n",
    "batch_size = 3\n",
    "\n",
    "# Tracks the score on the real (non-simulated) environment to determine when to stop\n",
    "reward_real = 0\n",
    "episode_real_count = 0\n",
    "num_episodes = 5000\n",
    "\n",
    "train_from_model = False\n",
    "\n",
    "observations = np.empty(0).reshape(0,dimen)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    observation = observation.reshape(1,-1)\n",
    "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
    "    action = 0 if policy > np.random.uniform() else 1\n",
    "\n",
    "    observations = np.vstack([observations, observation])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    if train_from_model:\n",
    "        observation, reward, done = step_model(sess, observations, action)\n",
    "    else:\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward_real += reward\n",
    "    \n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    if done or len(observations) > 300:\n",
    "        \n",
    "        if not train_from_model:\n",
    "            episode_real_count += 1\n",
    "        \n",
    "        reward_total.append(reward_sum)\n",
    "        disc_rewards = discount(rewards, standardize=True)\n",
    "        grads += sess.run(grads_p, feed_dict={input_x_p: observations,\n",
    "                                            input_y_p: actions,\n",
    "                                            advantages_p: disc_rewards})\n",
    "        \n",
    "        if num_episode % batch_size == 0:\n",
    "            sess.run(update_grads_p, feed_dict={W1_grad_p: grads[0], W2_grad_p: grads[1]})\n",
    "            grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "            \n",
    "            if (num_episode % (100 * batch_size) == 0):\n",
    "                print(\"Episode {} rewards: {}\".format(num_episode, reward_sum/batch_size))\n",
    "            \n",
    "            if episode_real_count > 0:\n",
    "                if (reward_real/episode_real_count >= 200):\n",
    "                    print(\"Episode {} Training complete with total score of: {}\".format(\n",
    "                            num_episode, reward_real/episode_real_count))\n",
    "                    break\n",
    "            \n",
    "            reward_sum = 0\n",
    "            \n",
    "        num_episode += 1\n",
    "        episode_real_count = 0\n",
    "        \n",
    "        observation = env.reset()\n",
    "        \n",
    "        observations = np.empty(0).reshape(0,dimen)\n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "        reward_real = 0\n",
    "        actions = np.empty(0).reshape(0,1)\n",
    "        \n",
    "        train_from_model = not train_from_model \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 166.0\n"
     ]
    }
   ],
   "source": [
    "# See our trained bot in action\n",
    "\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    observation = np.reshape(observation, [1, -1])\n",
    "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
    "    action = 0 if policy > 0.5 else 1\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
