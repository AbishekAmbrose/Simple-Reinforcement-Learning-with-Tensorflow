{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL\n",
    "Re-write of code from [Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-27 20:58:48,654] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2 # Learning rate, applicable to both nn, policy and model\n",
    "\n",
    "gamma = 0.99 # Discount factor for rewards\n",
    "\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad**2\n",
    "\n",
    "model_batch_size = 3 # Batch size used for training model nn\n",
    "policy_batch_size = 3 # Batch size used for training policy nn\n",
    "\n",
    "dimen = 4 # Number of dimensions in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount(r, gamma=0.99, standardize=False):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
    "    \"\"\"\n",
    "    discounted = np.array([val * (gamma ** i) for i, val in enumerate(r)])\n",
    "    if standardize:\n",
    "        discounted -= np.mean(discounted)\n",
    "        discounted /= np.std(discounted)\n",
    "    return discounted\n",
    "\n",
    "def step_model(sess, xs, action):\n",
    "    \"\"\" Uses our trained nn model to produce a new state given a previous state and action \"\"\"\n",
    "    # Last state\n",
    "    x = xs[-1].reshape(1,-1)\n",
    "    \n",
    "    # Append action\n",
    "    x = np.hstack([x, [[action]]])\n",
    "    \n",
    "    # Predict output\n",
    "    output_y = sess.run(predicted_state_m, feed_dict={input_x_m: x})\n",
    "    \n",
    "    # predicted_state_m == [state_0, state_1, state_2, state_3, reward, done]\n",
    "    output_next_state = output_y[:,:4]\n",
    "    output_reward = output_y[:,4]\n",
    "    output_done = output_y[:,5]\n",
    "    \n",
    "    # First and third env outputs are limited to +/- 2.4 and +/- 0.4\n",
    "    output_next_state[:,0] = np.clip(output_next_state[:,0],-2.4,2.4)\n",
    "    \n",
    "    output_next_state[:,2] = np.clip(output_next_state[:,2],-0.4,0.4)\n",
    "    \n",
    "    # Threshold for being done is likliehood being > 0.1\n",
    "    output_done = True if output_done > 0.01 or len(xs) > 500 else False\n",
    "    \n",
    "    return output_next_state, output_reward, output_done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Neural Network\n",
    "The model neural network will be used to map the current environment and action to a future environment, action and reward. This will serve as an approximation to the actual environment, since there may be instances where training on an a real environment would be too costly or time-consuming. This neural network will learn to replicate the environment and will be useful in training a policy\n",
    "### Architecture\n",
    "The architecture will consist of two layers with 256 neurons each with relu activations. There will be three output layers, one for next observation, one for reward and one for a trigger whether the game is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_hidden_m = 256\n",
    "\n",
    "# Dimensions of the previous state plus 1 for the action\n",
    "dimen_m = dimen + 1\n",
    "\n",
    "# Placeholder for inputs\n",
    "input_x_m = tf.placeholder(tf.float32, [None, dimen_m])\n",
    "\n",
    "# First layer\n",
    "W1_m = tf.get_variable(\"W1_m\", shape=[dimen_m, num_hidden_m],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B1M\")\n",
    "layer1_m = tf.nn.relu(tf.matmul(input_x_m, W1_m) + B1_m)\n",
    "\n",
    "# Second layer\n",
    "W2_m = tf.get_variable(\"W2_m\", shape=[num_hidden_m, num_hidden_m],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2_m = tf.Variable(tf.zeros([num_hidden_m]), name=\"B2_m\")\n",
    "layer2_m = tf.nn.relu(tf.matmul(layer1_m, W2_m) + B2_m)\n",
    "\n",
    "# Third (output) layers\n",
    "# Note that there are three separate output layers, \n",
    "# one for next observation, reward and whether the game is complete\n",
    "\n",
    "W_obs_m = tf.get_variable(\"W_obs_m\", shape=[num_hidden_m, 4],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_obs_m = tf.Variable(tf.zeros([4]), name=\"B_obs_m\")\n",
    "\n",
    "W_reward_m = tf.get_variable(\"W_reward_m\", shape=[num_hidden_m, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_reward_m = tf.Variable(tf.zeros([1]), name=\"B_reward_m\")\n",
    "\n",
    "W_done_m = tf.get_variable(\"W_done_m\", shape=[num_hidden_m,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "B_done_m = tf.Variable(tf.zeros([1]), name=\"B_done_m\")\n",
    "\n",
    "output_obs_m = tf.matmul(layer2_m, W_obs_m) + B_obs_m\n",
    "output_reward_m = tf.matmul(layer2_m, W_reward_m) + B_reward_m\n",
    "output_done_m = tf.sigmoid(tf.matmul(layer2_m, W_done_m) + B_done_m)\n",
    "\n",
    "# Placeholders for inputs used in training\n",
    "actual_obs_m = tf.placeholder(tf.float32, [None, dimen_m], name=\"actual_obs\")\n",
    "actual_reward_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_reward\")\n",
    "actual_done_m = tf.placeholder(tf.float32, [None, 1], name=\"actual_done\")\n",
    "\n",
    "# Putting it all together\n",
    "predicted_state_m = tf.concat([output_obs_m, output_reward_m, output_done_m], axis=1)\n",
    "\n",
    "# Loss functions\n",
    "loss_obs_m = tf.square(actual_reward_m - output_reward_m)\n",
    "loss_reward_m = tf.square(actual_reward_m - output_reward_m)\n",
    "loss_done_m = -tf.log(actual_done_m * output_done_m + \n",
    "                (1 - actual_done_m) * (1 - output_done_m))\n",
    "\n",
    "# Model loss is simply the average loss of the three outputs\n",
    "loss_m = tf.reduce_max(loss_obs_m + loss_reward_m + loss_done_m)\n",
    "\n",
    "adam_m = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "update_m = adam_m.minimize(loss_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Neural Network\n",
    "The policy network will be used to determine policy. We will use a combination of a simulated environment from the model neural network we created and a real environment.\n",
    "### Architecture\n",
    "Our architecture will consist of one hidden layer with 10 neurons and an output layer used to determine the policy (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden_p = 10 # Number of hidden units in the nn used to determine policy\n",
    "\n",
    "input_x_p = tf.placeholder(tf.float32, [None, dimen], name=\"input_x\")\n",
    "\n",
    "# First layer\n",
    "W1_p = tf.get_variable(\"W1\", shape=[dimen,num_hidden_p], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1_p = tf.nn.relu(tf.matmul(input_x_p, W1_p))\n",
    "\n",
    "# Second layer\n",
    "W2_p = tf.get_variable(\"W2\", shape=[num_hidden_p, 1], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "output_p = tf.nn.sigmoid(tf.matmul(layer1_p, W2_p))\n",
    "\n",
    "# Placeholders for inputs used in training\n",
    "input_y_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"input_y\")\n",
    "advantages_p = tf.placeholder(tf.float32, shape=[None,1], name=\"reward_signal\")\n",
    "\n",
    "# Loss function\n",
    "# Below is equivalent to: 0 if input_y_p == output_p else 1\n",
    "log_lik_p = tf.log(input_y_p * (input_y_p - output_p) + \n",
    "                 (1 - input_y_p) * (input_y_p + output_p))\n",
    "\n",
    "# We'll be trying to maximize log liklihood\n",
    "loss_p = -tf.reduce_mean(log_lik_p * advantages_p)\n",
    "\n",
    "# Gradients\n",
    "W1_grad_p = tf.placeholder(tf.float32,name=\"W1_grad\")\n",
    "W2_grad_p = tf.placeholder(tf.float32,name=\"W2_grad\")\n",
    "batch_grad_p = [W1_grad_p, W2_grad_p]\n",
    "trainable_vars_p = [W1_p, W2_p]\n",
    "grads_p = tf.gradients(loss_p, trainable_vars_p)\n",
    "\n",
    "# Optimizer\n",
    "adam_p = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Update function\n",
    "update_grads_p = adam_p.apply_gradients(zip(batch_grad_p, [W1_p, W2_p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50592601"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run(output_done_m,feed_dict={input_x_m: np.random.random(size=(1,5))})[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Model neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300 Training model loss: 0.0017830206779763103\n",
      "Episode: 600 Training model loss: 0.0018709535943344235\n",
      "Episode: 900 Training model loss: 0.004871220327913761\n",
      "Episode: 1200 Training model loss: 0.0002028268645517528\n",
      "Episode: 1500 Training model loss: 2.101302561641205e-05\n",
      "Episode: 1800 Training model loss: 3.113907587248832e-05\n",
      "Episode: 2100 Training model loss: 1.992057150346227e-05\n",
      "Episode: 2400 Training model loss: 0.0016786165069788694\n",
      "Episode: 2700 Training model loss: 0.00012370661715976894\n",
      "Episode: 3000 Training model loss: 3.570126500562765e-05\n",
      "Episode: 3300 Training model loss: 7.034925147308968e-06\n",
      "Episode: 3600 Training model loss: 0.00032430628198198974\n",
      "Episode: 3900 Training model loss: 4.487152909860015e-05\n",
      "Episode: 4200 Training model loss: 1.5351017282227986e-05\n",
      "Episode: 4500 Training model loss: 0.0002187254576710984\n",
      "Episode: 4800 Training model loss: 5.91951102251187e-05\n"
     ]
    }
   ],
   "source": [
    "# Setup a session and init all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# We'll run up to 5000 episodes\n",
    "num_episode = 0\n",
    "num_episodes = 5000\n",
    "\n",
    "# Setup array to keep track of preivous states and outputs\n",
    "observations = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "dones = np.empty(0).reshape(0,1)\n",
    "\n",
    "draw_from_model = False\n",
    "\n",
    "# Placeholder for gradients. Gradients should be same shape as variables we're training\n",
    "grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    observation = np.reshape(observation, (1,-1))\n",
    "    \n",
    "    # Chose a random action\n",
    "    action = np.random.choice(range(env.action_space.n))\n",
    "    \n",
    "    # Append observation and actions\n",
    "    observations = np.vstack([observations, observation])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Get next observation based on our action\n",
    "    if draw_from_model:\n",
    "        # Branko\n",
    "        observation = sess.run(output_obs_m, feed_dict={input_x_m: observations})[0]\n",
    "        reward = sess.run(output_reward_m, feed_dict={input_x_m:observations})[0][0]\n",
    "        done = sess.run(output_done_m, feed_dict={input_x_m:observations})[0][0]\n",
    "\n",
    "    else:\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Keep track or rewards and dones\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    dones = np.vstack([dones, done])\n",
    "    \n",
    "    # If game is over\n",
    "    if done or len(observations) > 300:\n",
    "        \n",
    "        # Count the episode\n",
    "        num_episode += 1\n",
    "\n",
    "        # Previous state and actions for training model\n",
    "        states = np.hstack([observations, actions])\n",
    "        prev_states = states[:-1,:]\n",
    "        next_states = states[1:, :]\n",
    "        next_rewards = rewards[1:, :]\n",
    "        next_dones = dones[1:, :]\n",
    "        \n",
    "        # Our feed dictionary will consist of our previous state and \n",
    "        # we'll be solving for the next state, next reward and next done\n",
    "        if not draw_from_model:\n",
    "            feed_dict = {input_x_m: prev_states.astype(np.float32), \n",
    "                         actual_obs_m: next_states.astype(np.float32),\n",
    "                        actual_done_m: next_dones.astype(np.float32),\n",
    "                        actual_reward_m: next_rewards.astype(np.float32)}\n",
    "            loss, _ = sess.run([loss_m, update_m], \n",
    "                                             feed_dict=feed_dict)\n",
    "            \n",
    "        observation = env.reset()\n",
    "        \n",
    "        # If the batch is full\n",
    "        if num_episode % model_batch_size == 0:\n",
    "            \n",
    "            # Keep track of how we're doing\n",
    "            if (num_episode % (100 * model_batch_size) == 0):\n",
    "                print(\"Episode: {} Training model loss: {}\".format(num_episode, loss))\n",
    "\n",
    "            # Good enough, quit\n",
    "            if loss < 1e-6:\n",
    "                print(\"Training done! Episode: {} Model loss: {}\".format(num_episode, loss))\n",
    "                break\n",
    "                            \n",
    "            # Reset everything\n",
    "            observations = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            rewards = np.empty(0).reshape(0,1)\n",
    "            dones = np.empty(0).reshape(0,1)\n",
    "            grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "            \n",
    "#             if num_episode > 100:\n",
    "                # Begin drawing from model\n",
    "#                 draw_from_model = not draw_from_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300 rewards: 52.0\n",
      "Episode 600 rewards: 242.0\n",
      "Episode 900 rewards: 226.33333333333334\n",
      "Episode 918 Training complete with total score of: 301.0\n"
     ]
    }
   ],
   "source": [
    "# Keep track our our rewards\n",
    "reward_sum = 0\n",
    "\n",
    "# Tracks the score on the real (non-simulated) environment to determine when to stop\n",
    "reward_real = 0\n",
    "episode_real_count = 0\n",
    "num_episodes = 5000\n",
    "\n",
    "# Trigger used to decide whether we should train from model or from real environment\n",
    "train_from_model = False\n",
    "\n",
    "# Setup array to keep track of observations, rewards and actions\n",
    "observations = np.empty(0).reshape(0,dimen)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Gradients\n",
    "grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    observation = observation.reshape(1,-1)\n",
    "    \n",
    "    # Determine the policy\n",
    "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
    "    \n",
    "    # Decide on an action based on the policy, allowing for some randomness\n",
    "    action = 0 if policy > np.random.uniform() else 1\n",
    "\n",
    "    # Keep track of the observations and actions\n",
    "    observations = np.vstack([observations, observation])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    # Determine next observation either from model or real environment\n",
    "    if train_from_model:\n",
    "        observation, reward, done = step_model(sess, observations, action)\n",
    "    else:\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        # Keep track of real rewards to determine when to stop\n",
    "        reward_real += reward\n",
    "    \n",
    "    # Keep track of rewards\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    dones = np.zeros(shape=(len(observations),1))\n",
    "    \n",
    "    # If game is over or running long\n",
    "    if done or len(observations) > 300:\n",
    "        \n",
    "        # Keep track of how many real scenarios to determine average score from real environment \n",
    "        if not train_from_model:\n",
    "            episode_real_count += 1\n",
    "             # Previous state and actions for training model\n",
    "                \n",
    "            states = np.hstack([observations, actions])\n",
    "            prev_states = states[:-1,:]\n",
    "            next_states = states[1:, :]\n",
    "            next_rewards = rewards[1:, :]\n",
    "            next_dones = dones[1:, :]\n",
    "\n",
    "            feed_dict = {input_x_m: prev_states.astype(np.float32), \n",
    "                         actual_obs_m: next_states.astype(np.float32),\n",
    "                        actual_done_m: next_dones.astype(np.float32),\n",
    "                        actual_reward_m: next_rewards.astype(np.float32)}\n",
    "\n",
    "            loss, _ = sess.run([loss_m, update_m], feed_dict=feed_dict)\n",
    "            \n",
    "        \n",
    "        # Discount rewards\n",
    "        disc_rewards = discount(rewards, standardize=True)\n",
    "        \n",
    "        # Add gradients to running batch\n",
    "        grads += sess.run(grads_p, feed_dict={input_x_p: observations,\n",
    "                                            input_y_p: actions,\n",
    "                                            advantages_p: disc_rewards})\n",
    "        \n",
    "        num_episode += 1\n",
    "        \n",
    "        observation = env.reset()\n",
    "\n",
    "        # Reset everything\n",
    "        observations = np.empty(0).reshape(0,dimen)\n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "        actions = np.empty(0).reshape(0,1)\n",
    "        \n",
    "        # Toggle between training from model and from real environment\n",
    "#         train_from_model = not train_from_model \n",
    "\n",
    "        # If batch full\n",
    "        if num_episode % policy_batch_size == 0:\n",
    "            \n",
    "            # Update gradients\n",
    "            sess.run(update_grads_p, feed_dict={W1_grad_p: grads[0], W2_grad_p: grads[1]})\n",
    "            \n",
    "            # Reset gradients\n",
    "            grads = np.array([np.zeros(var.get_shape().as_list()) for var in trainable_vars_p])\n",
    "            \n",
    "            # Print periodically\n",
    "            if (num_episode % (100 * policy_batch_size) == 0):\n",
    "                print(\"Episode {} rewards: {}\".format(num_episode, reward_sum/policy_batch_size))\n",
    "            \n",
    "            # If our real score is good enough, quit\n",
    "            if episode_real_count > 0:\n",
    "                if (reward_sum/policy_batch_size >= 300):\n",
    "                    print(\"Episode {} Training complete with total score of: {}\".format(\n",
    "                            num_episode, reward_sum/policy_batch_size))\n",
    "                    break\n",
    "                episode_real_count = 0\n",
    "                reward_real = 0\n",
    "                \n",
    "            reward_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 1234.0\n"
     ]
    }
   ],
   "source": [
    "# See our trained bot in action\n",
    "\n",
    "observation = env.reset()\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    observation = np.reshape(observation, [1, -1])\n",
    "    policy = sess.run(output_p, feed_dict={input_x_p: observation})\n",
    "    action = 0 if policy > 0.5 else 1\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
